---
title: "EDA Capstone"
author: "Willem van der Schans"
date: "2/3/2021"
output:
  pdf_document:
    fig_height: 4
    fig_width: 8
    highlight: kate
    toc: no
    toc_depth: 4
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: kate
    number_sections: no
    theme: paper
    toc: yes
    toc_float: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = FALSE)
options(java.parameters = c("-XX:+UseConcMarkSweepGC", "-Xmx8192m"))
#options(java.parameters = "-Xmx8g")
```

```{r, echo=F, message=FALSE, warning=FALSE}
# Operational Packages
library(dplyr) # Additional Base level functionality of R
library(ggplot2) # Comprehensive Graphical Output Package
library(ggthemes) # More themes for GGPLOT2
library(Rfast) # Fast R Functions
library(bestNormalize) # Automatic Optimal Normalization
library(rlist) #list.append
library(scales) # Control over Axis formatting
library(kableExtra) # Output HTML Formatted Tables
library(reshape2) # Melt Function Correlation Matrix
library(lsr) #Correlation Matrix
library(e1071) #Skewness and other depreciated functions
library(stats)
library(lubridate) # For working with dates
library(gridExtra) # Plot in Grids

library(caret) # MachineLearning package
library(doSNOW) # Parralel processing
library(rminer) #Mmetric Function'


library(tidyverse)
library(ggplot2)
library(reshape2)
library(ggthemes) #General Theme Package
library(gghighlight)

library(xtable)
library(papeR)
library(reporttools)


```

**Note to Team: We need to focus on everything above feature engineering and machine learning, the way we are predicting in this document makes no sense as we cannot predict anything right now. We need to transmute the data and see if we can find information to use through other statistical means.**

# Project Introduction

Maverik convenience stores must have accurate sales models for the inventory held at each location. Numerous factors affect individual stores or groups of stores calculations of these forecasting models, including location, size of the store, day of the week, holidays, weather, and many more variables. Maverik also has the challenge of determining whether they fulfill the customer's needs by carrying the items the Maverik customers want.

The company would like to improve the sales forecasting model of candy bars. Currently, Maverik does not have a model for forecasting item-level sales in these categories. A Maverik team member will be building out a model simultaneously to the capstone and will provide an update on how their model is going. The capstone model and the Maverik model will be compared and evaluated on the RMSE performance.

Maverik has provided 1.4 million rows of historical sales data beginning in September 2017 through August 2020, and training of the model will use this historical data set. The project goal is to build a forecasting model that can be easily integrated into Maverik's current workflow and improves their current system of managing candy bar forecasts by enabling better management of candy bar inventory, pricing, budgeting, and forecasting.

Maverik must have sound forecasting models for the different items they carry in the stores to generate accurate budgets and financial forecasts at their convenience store locations. In this project, the goal is to build a precise forecasting model of candy bar sales.

[**!! Check if old!!**]{.ul}

# Analytics Problem

For the project, the capstone team will be building a forecasting model to predict candy bar  sales. The dataset provided contains data points from a subset of Maverik's convenience  stores. It includes several variables that will be analyzed to determine which variables  significantly influence candy bar sales. We know the data is separated by day per candy bar  sold from the initial meeting with the Maverik team. Maverick must forecast a point estimate for  candy sales to support inventory decisions. Within the dataset.

\newpage

## CodeBook: Main Dataset

-   Variable_Name Data_Type Description 

-   SITE_ID INT = Unique Site Identifier (Maverik Store ID) 

-   LOCATION_ID INT = Location Identifier, 1:1 with SITE_ID 

-   SQ_FOOTAGE INT = Square Footage of Store (Indoor Footprint) 

-   LOCALE CHR = Geospatial Descriptor 

-   MPDS INT = Count of Fueling Positions 

-   DATE DATE = Day Date 

-   DAY_OF_WEEK CHR = Day of Week Descriptor 

-   HOLIDAY CHR = Holiday Description, If Applicable 

-   HCA_GBV NUM = Gross Book Value of Site 

-   MAX_TEMP NUM = Daily Maximum Temperature 

-   MIN_TEMP NUM = Daily Minimum Temperature 

-   PROJECT CHR = If Day's Sales Was Impacted by a Project 

-   ITEM_ID INT = Item Identifier 

-   SALES INT = Sum of Sales 

-   QUANTITY_SOLD = INT Sum of Units

### CodeBook: Products Mapping Subset

-   Item_ID = Item Identifier 

-   Item_Desc = Item Description per Identifier

-   Department_Decs = Product department description

\newpage

# Method

## Preliminary Analysis

The data supplied comes in two data sets; A main data set and a supplemental data set in the form of a dictionary for the item_id's in the main data set. The supplemental dataset is provided to provide further understanding of sales number to allow for a proper assessment of data validity. The aim is to join the two data sets by mapping Item_description values with Item_ID values and to create a new variable in the main dataset named Item_Desc. After the value mapping has been completed the supplementary dataset will be discarded since all information will be extracted, due to Department_Desc being a one-leveled factor variable.

```{r}
download.file(
  "https://github.com/Kydoimos97/CapstoneMSBA2020/raw/main/Data/CapstoneProjectInfoRevised.rds", 
  destfile = "CapstoneProjectInfoRevised.rds")

download.file(
  "https://github.com/Kydoimos97/CapstoneMSBA2020/raw/main/Data/CapstoneProjectProducts.rds", 
  destfile= "CapstoneProjectProducts.rds")

df <- readRDS("CapstoneProjectInfoRevised.rds")
products <- readRDS("CapstoneProjectProducts.rds")

# Factorization
df$Site_ID <- as.factor(df$Site_ID)
df$Location_ID <- as.factor(df$Location_ID)
df$Locale <- as.factor(df$Locale)
df$Fiscal_Period <- as.factor(df$Fiscal_Period)
df$MPDS <- as.factor(df$MPDS)
df$Project <- as.factor(df$Project)

# Numeration
df$Quantity_Sold <- as.numeric(df$Quantity_Sold)
df$SQ_Footage <- as.numeric(df$SQ_Footage)# Shows Factor like tendencies
df$Periodic_GBV <- as.numeric(df$Periodic_GBV)
df$Current_GBV <- as.numeric(df$Current_GBV)

# Re-origin of Dates
x <- min(df$Open_Date) 
df$Open_Date <- as_date(df$Open_Date, origin = x)
df$DATE <- as_date(df$DATE, origin = x)

rm(x)

```

```{r, warning=FALSE, message=FALSE}
candy_vector <- c(products$Item_Desc)

id_vector <- c(products$Item_ID)

df$Item_desc <- df$Item_ID
df$Item_desc <- plyr::mapvalues(df$Item_desc, id_vector, candy_vector)

df$Item_ID <- as.factor(df$Item_ID)

# Reodering Data
df <- df[, c(1,2,6,17,7,3,12,13,14,10,11,8,9,15,16,4,5)]

#Rename Variables
names(df) <- tolower(make.names(names(df)))

rm(products, candy_vector, id_vector)
```

```{r, results="asis"}
# summarize numeric variables
tableContinuous(df[,sapply(df, is.numeric)], 
                stats = c("n", "min", "q1", "median", "mean", "q3", "max", "na"),
                cap = "Summary of Numeric variables")
```

Table 1 shows that after mapping the Item_desc variables to the main dataset the dataset consists of 1352124 observations over 17 variables. Missing data is only shows in the temperature data where 11799 entries are missing. Since to locational data is available research need to be done on how to impute these missing values. Since only 10 site_id's exist it is possible to interpolate the location of these sites by matching historical temprature data with the given temprature data, and the knowledge that the area of operations lies around utah severely limiting the scope, however the problem is the cost of accessing relevant API's is over \$100 dollar per month. Webscraping is an option however a legal grey area especially since this project is conducted for a corperate sponsor. Further options for imputation have to be explored and will likely be based on a form of bootstrapping data based on timeseries and previous years or removal of temprature data overall.

\newpage

```{r, results="asis"}
# summarize non-numeric variables
tableNominal(df[c(12,13,14)], cap = "Summary of Numeric variables", cumsum = FALSE)
```

Looking at table 2 it shows that Quantity sold and sales seem to have extremely long right tails. Since Sales is the target variable this needs to be further investigated and looked at to ensure optimal algorithm performance. The Project variable holds limited information since only 0.02% of the data entries deviate from the baseline of NONE.

\newpage

## Multicolinnearity

Multicolinnearity has a negative effect on model performance and therefore needs to be elimited. The negative effects on model performance originates from two main points. First multicollinearty makes coefficients extyremely sensitive effectively eliminating the ability to generalize a model. Secondly multicollinearty can misrepresent the P-value therefore making it hard to rely on model results to rejust or retain null-hypothesises. Since both of these two underlying problems would have a detrimental effect on the level of trust that can be put on the model outcome to be correct multicollinearity will be elimited in the final product. In this current EDA we will be looking at where multicollinearity exists and what engineering to consider for feature engineering.

To eliminate Multicollinearity we will follow the guidelines of correlation below and will eliminate any strong relationship of an absolute r above 0.7. (Mindrila & Balentyne).

**Absolute Value of r Strength of Relationship**

r \< 0.3 None or very weak

0.3 \< r \< 0.5 Weak

0.5 \< r \< 0.7 Moderate

r \> 0.7 Strong

Note: that Na's are omitted however testing showed that this does not change results at all, but merely allows the correlation matrix to be complete instead of showing gaps for correlations regarding temprature data.

### Corelation Matrix: Pre-Engineering

```{r, fig.cap='Pre-engineering, Correlation Matrix'}
df2 <- df
df2$open_date <- as.numeric(df2$open_date)
df2$date <- as.numeric(df2$date)

nums <- unlist(lapply(df2, is.numeric))
Integers <- na.omit(df2[ , nums]) # Remove NA's due to temp

melted_cormat <- melt(cor(Integers))

names(melted_cormat)[3] <- "Correlation"

melted_cormat <- melted_cormat %>% 
    arrange(Var1) %>%
    group_by(Var1) %>%
    filter(row_number() >= which(Var2 == Var1))

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=Correlation)) + 
  geom_tile(aes(fill=Correlation), col = "Black") + 
  theme_fivethirtyeight () +
  theme(legend.position="right",legend.direction = "vertical",
        legend.text = element_text(size=7)) +
  geom_text(label = round(melted_cormat$Correlation, digits = 2),
            color="Black", size=3, alpha=0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1, size=10),
        axis.text.y = element_text(angle=0,vjust = 0.5, hjust=1, size=10)) +
  scale_fill_gradient2_tableau("Classic Orange-White-Blue Light", limits = c(-1,1)) +
  theme(axis.title = element_text()) + 
  ylab("Feature") + 
  xlab("Feature") + 
  labs(title = "Correlation Matrix")
  

rm(Integers, melted_cormat, nums, df2)
```

As shown in figure , most multicollinearity shows between the max and min temperature variables. While the makes sense because min and max temp move closely together in any direction, we need to consider if retaining one variable would provide better model outcome since this data largely holds the same information. We can remedy this multicollinearity by making a temprature range variable by substracting mintemp from maxtemp, after which we remove mintemp. The reason is that mintemp is often a temprature reached during the night when the bulk of customers do not visit gasstations. A focus on maxtemp means a focus on what happens during the day, when if the weather is nice more people are out.

Next we can see that the GBV variables heavily correlate with each other and sq_footage. This makes sense with GBV standing for gross book value it is logical that a large store has a larger Gross book value. this relationship can be eliminated by creating two new variables. The first variable being cgbv_sqf which stands for current gbv per square footage, this variable shows the valuation per store and removes the information inhereted for the sq_footage variable. After this a variable should be made that shows the difference between the current and periodic gbv by subtracting the latter from the first. This shows a difference and again removes the inhereted data from square footage. After this current and periodic_gbv will be removed.

Sq_footage behaves like a factor variable is currently removed but needs a different treament "12", it heavily correlates with open_date as expected since the data set only contains 10 stores and the sq_footage hardly flucuates, and the open_date always remains the same \<- removed open date and added a days_open variable (useless?)

### Removal and Creation of variables

```{r}
# creation of tempdiff
df$temp_diff <- as.numeric(df$maxtemp-df$mintemp)

# Creation of cgbv_sqf
df$cgbv_sqf <- as.numeric(df$current_gbv/df$sq_footage)

# creation of diff_gbv
df$diff_gbv <- df$current_gbv - df$periodic_gbv

#Days open
x <- min(df$open_date) 
df$days_open <- as.numeric(df$date-df$open_date)

rm(x)

df <- df[, c(1,2,3,4,21,6,7,19,20,10,18,12,13,14,15,16,17,8,9,11,5)]

df <- df[,-c(12,18,19,20,21)]
```

### Corelation Matrix: Post-Engineering

```{r, fig.cap='Post-engineering, Correlation Matrix'}
df2 <- df
df2$date <- as.numeric(df2$date)

nums <- unlist(lapply(df2, is.numeric))
Integers <- na.omit(df2[ , nums]) # Remove NA's due to temp

melted_cormat <- melt(cor(Integers))

names(melted_cormat)[3] <- "Correlation"

melted_cormat <- melted_cormat %>% 
    arrange(Var1) %>%
    group_by(Var1) %>%
    filter(row_number() >= which(Var2 == Var1))

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=Correlation)) + 
  geom_tile(aes(fill=Correlation), col = "Black") + 
  theme_fivethirtyeight () +
  theme(legend.position="right",legend.direction = "vertical",
        legend.text = element_text(size=7)) +
  geom_text(label = round(melted_cormat$Correlation, digits = 2),
            color="Black", size=3, alpha=0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1, size=10),
        axis.text.y = element_text(angle=0,vjust = 0.5, hjust=1, size=10)) +
  scale_fill_gradient2_tableau("Classic Orange-White-Blue Light", limits = c(-1,1)) +
  theme(axis.title = element_text()) + 
  ylab("Feature") + 
  xlab("Feature") + 
  labs(title = "Correlation Matrix")
  

rm(Integers, melted_cormat, nums, df2)
```

As shown in figure 2 the deployed strategies elimited all occurences of moderate to strong multicollinearity. only weak multicollinearity remains allowing algoritmhs to perform most optimally.

\newpage

## Outlier Detection and Relationships

### Outlier Detection and Relationships:Numerical Variables seperated by Site_ID

```{r, fig.height = 9, fig.width = 7, fig.cap='Relationships between Target and Numeric Variables seperated by Site_ID', warning=FALSE, message=FALSE}
num_list <- colnames(df[sapply(df, is.numeric)])
num_list <- num_list[-c(4,5,6,7)]
plot_list <- list(1)

counter = 0

resample <- caret::createDataPartition(y=df$sales, p = 0.01, list=FALSE) #This is due to a limitation on PDF's
df2 <-df[resample,]

mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

for (i in num_list) {
  
  counter = counter+1
  x = ggplot(data=na.omit(df2[df2$sales < 100,]), aes_string(x = "sales", y = i, color="site_id")) +
      geom_point(position = "identity", stat = "identity", alpha = .35, size=1, stroke=.05) + 
      theme_fivethirtyeight() + 
      scale_color_manual(values = mycolors) +
      scale_y_continuous(labels = comma) + scale_x_continuous(labels = dollar) + coord_flip() + 
      ggtitle(paste0("sales vs ", as.character(i), sep= " ")) + theme(plot.title = element_text(size=10), legend.position = "right", legend.direction = "vertical") + 
      guides(color = guide_legend(override.aes = list(size=2.5, alpha = 1) ,title = "Legend", title.position = "top"))
  
  
  assign(paste0("plt",i), x)
  plot_list <- rlist::list.append(plot_list, x)
}

plot_list <- plot_list[-1]

nCol <- floor(sqrt(length(plot_list)))

grid.arrange(grobs=plot_list, widths = c(1,1), ncol=2, layout_matrix =  rbind(c(1,1),
                                                                                 c(2,2), 
                                                                                 c(3,3)), 
             top = "Relationships between Target and Numeric Variables seperated by Site_ID")

for (i in num_list) {
  rm(list = paste0("plt",i))
}

rm(plot_list, counter, num_list, x, i, nCol, df2, mycolors, resample)
```

Figure 3 shows the relationships between the target variable sales and a number of numeric variables. ALl of these numeric variables are location bound and therefore Colored by Site_ID. When looking at days_open, a clear correlation is shown which Site_ID. This relationship is logical and obvious due to each location opening on a different day. No clear relationship is shown between sales and days_open.

Sales vs current gross book value per square foot shows mostly that current book value per site does not seem to change at all in the data set. Apart from this no clear relationship is shown between sales and current gross book value per square foot

Sales versus difference in gross book value between current and **Figure this one out**.

**Figure 3 might need an overhaul, thinking of a bar chart however this data is near impossible to plot do I am not sure if it will get any better then this**

\newpage

### Outlier Detection and Relationships:Other Numerical Variables

```{r, fig.height = 9, fig.width = 7, fig.cap='Relationships between Target and Numeric Variables'}
num_list <- colnames(df[sapply(df, is.numeric)])
num_list <- num_list[-c(1,2,3,7)]
plot_list <- list(1)

counter = 0
colcounter = 0

resample <- caret::createDataPartition(y=df$sales, p = 0.01, list=FALSE)
df2 <-df[resample,]

mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

for (i in num_list) {
  
  counter = counter+1
  colcounter = counter*3
  x = ggplot(data=na.omit(df2[df2$sales < 150,]), aes_string(x = "sales", y = i)) +
      geom_point(position = "identity", stat = "identity", alpha = .75, color="grey", size=0.75)  + scale_y_continuous(labels = comma) + scale_x_continuous(labels= dollar) + 
      geom_smooth(method = lm, se=TRUE, color=mycolors[colcounter], size=1, fullrange = TRUE) + ggtitle(paste0("sales vs ", as.character(i), sep= " ")) +
      theme_fivethirtyeight() + theme(plot.title = element_text(size=10)) 
  
  
  assign(paste0("plt",i), x)
  plot_list <- rlist::list.append(plot_list, x)
}

plot_list <- plot_list[-1]

nCol <- floor(sqrt(length(plot_list)))

grid.arrange(grobs=plot_list, widths = c(1,1), ncol=2, layout_matrix =  rbind(c(1,1),
                                                                                 c(2,2), 
                                                                                 c(3,3)), 
             top = "Relationships between Target and Numeric Variables")


for (i in num_list) {
  rm(list = paste0("plt",i))
}

rm(plot_list, counter, num_list, x, i, nCol, df2, resample, mycolors, colcounter)
```

Figure 4 shows a slight positive relationship between maxtemp and sales. With higher tempratures causing more candy bar sales across locations. The temprature difference shows almost no relationship with sales showing the temprature range is not what drives customers to buy candy bars. Lastly sales and quantity sold show a clear relationship as expected, since more quantity sold results in a increase in sales.

\newpage

### Outlier Detection and Relationships: Categorical Variables

In this section boxplot are created and accompanying tables to precisely read the data. These variables will need to be evaluated later down the line by aggregating all sales on a day and rerunning the code below to find differences between days instead of products and see if factor levels show different key_metrics then.

#### Categorical Variables: Locale

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and Locale'}
i = "locale"
o <- c(summary(df$locale))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() +   ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + theme_fivethirtyeight() + theme(axis.text.x = element_text(angle =0)) + scale_y_discrete(guide = guide_axis(n.dodge=2))+
  theme(plot.title = element_text(size = 10, face = "bold")) + scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
  
  y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Sales and Locale between different factor levels"), comment = FALSE)

```

Figure 5 and Table 3 Shows only slight differences between different factor levels of sales and locale. The sample size of each factor level is semi well distributed with only Suburban/residential only deviating a lot from the average sample size. With industrial having a higher sales median of 3.18 over 2.79 and lower for the rest of the factor levels, it warrants more research to figure out what products warrant these extra sales.

\newpage

#### Categorical Variables: MPDS

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and MPDS'}
i = "mpds"
o <- c(summary(df$mpds))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() +   ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + theme_fivethirtyeight() + theme(axis.text.x = element_text(angle =0)) + scale_y_discrete(guide = guide_axis(n.dodge=1))+
  theme(plot.title = element_text(size = 10, face = "bold")) + scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
  
  y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Sales and MPDS between different factor levels"), comment = FALSE)

```

In figure 6 and table 4 no clear relationship can be found between the amount of mpds and sales. The lack of a relationship shows that sales of indiviual items are more site specific and not based on the amount of pumps present at the gas station. \newpage

#### Categorical Variables: Project

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and Project'}
i = "project"
o <- c(summary(df$project))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() +   ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + theme_fivethirtyeight() + theme(axis.text.x = element_text(angle =0)) + scale_y_discrete(guide = guide_axis(n.dodge=2))+
  theme(plot.title = element_text(size = 10, face = "bold")) + scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
  
  y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Project and MPDS between different factor levels"), comment = FALSE)

  
rm(i, o, y, x)
```

Table 5 and figure 7 show no clear relationship between factor levels of project. Both factor levels apart from NONE have an extremely small sample size and generalization should happen carefully. Program enhancement seems to result in a lower median sales, which can be explained due to the fact that enhancement likely means that existing parts of the building have had to close, as opposed to none and addition, interefering with the ability of a location to sell products.

\newpage

#### Outlier Detection and Relationships: Time Series of Sales per Site_ID aggregated per day

```{r}
sum_sales <- aggregate(df$sales, by=list(df$date, df$site_id), FUN=sum)
names(sum_sales) <- c("Date", "Site_ID", "Sum_of_Sales")
```

```{r, fig.height = 9, fig.width = 7, fig.cap='Sales over time'}
mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

ggplot(sum_sales, aes(x=Date, y=Sum_of_Sales, color=Site_ID)) +
  facet_wrap(~Site_ID, scale="free_y", nrow = 5, ncol = 2) +
  geom_point(position = "identity", stat = "identity", alpha = .8, color="grey", size=0.75) +
  geom_smooth(se=F) +
  theme_fivethirtyeight() +
  scale_color_manual(values = mycolors) +
  scale_y_continuous(labels = scales::dollar, limits = c(0,1500)) +
  theme(legend.position = ("none"), strip.text = element_text(size=15), plot.title = element_text(size=22),axis.title.x = element_text(size = 15), axis.title.y =     element_text(size = 15)) +
  labs(x = "Site_ID", y = "Occupation Growth in percentage", title = "Total Sales per Site_ID ")

```

```{r, results='asis'}
counter = 0

list_siteid <- NULL
list_mpds <- NULL
df2 <- NULL

for (i in c(levels(df$site_id))) {
   counter = counter+1
   list_siteid[counter] <- i
   df2 <- df %>% filter(site_id == i)
   list_mpds[counter] <- mean(as.numeric(as.character(df2$mpds)))
  
}

table2 <-data.frame(site_id = list_siteid, mpds = list_mpds)

xtable(table2, caption = "Amount of Pumps per Site_ID")

rm(table2, list_mpds, list_mpds, i, counter, df2)

```

Figure 8 shows the aggregate sum of sales per day seperated per site_id, or location. A clear difference can be seen between different sites in the amount of aggregated sales per day. Every location also shows differences in seasonality, e.g. 380 vs 280, and growth e.g. 280 vs 517. Table 6 shows the site_id and the matching amount of pumps. As of now no clear difference can be seen in the amount of pumps and the amount of sales in a given day. Seasonality also does not seem to depend on the amount of pumps available at the get station and neither does growth.

\newpage

# Feature Engineering [Possibly Needs to be removed]

**We need to rethink the whole data set and aggregate per day to be able to predict anything, predicting on ITEM_ID level is impossible due to the extremely limited sample, size. Any suggestions would be welcome team**

## Dummy Encoding

```{r}
df_bu <- df

df_target_bu <- df$sales

df <- df[-c(1,4)] # Variables removed for performance purposes

df <- df[-c(4,6,10,11)] #variables removed due to singularities <- "explain" Days_open = Date, Locale and Mpds = Site_ID, why cgbv = same as Site (gbv never changes)

df <- df[-c(1,9)] #removed because of extreme bias

df <- na.omit(df)
df_target_bu <- df$sales

#df$item_id <- as.numeric(as.character(df$item_id)) #for performance
df$fiscal_period <- as.numeric(as.character(df$fiscal_period)) #for performance


df <- dummyVars(sales~., data=df, fullRank = TRUE) %>% 
  predict(newdata = df)


df <- as.data.frame(df)
df$sales <- df_target_bu

set.seed(500)
inTrain <- createDataPartition(y=df$sales, p = 0.75, list=FALSE)
train <-df[inTrain,-length(names(df))]
train_target<-df[inTrain,length(names(df))]
```

\newpage

# Machine Learning: Variable Selection [Possibly Needs to be removed]

Why logistic regression

1.  shows variable coefficients and can be used for variable selection etc etc.

## Logistic Regression

```{r}
system.time(glm_model <- glm(train_target~., data=train))

summary(glm_model)

```

Take away --\> it is not predicting anything at all, and it always wrong.

#### Multicolinearity

Below we can see the VIF scores of the logistic regression model's independent variables. The lower the VIF, the better as long as VIF is lower than 10, there is no cause for major concern (Glen, 2020). Given the VIF scores, the Logistic Regression model passes the multicolinearity assumption. \<\_ Oh no!

```{r}
xtable(as.data.frame(car::vif(glm_model)))
```
