---
title: "EDA Capstone"
author: "Willem van der Schans, Kelsey Metivier, Sam Erickson, Tim Gain"
date: "2/3/2021"
output:
  pdf_document:
    fig_height: 4
    fig_width: 8
    highlight: kate
    toc: no
    toc_depth: 4
---

\newpage

\tableofcontents

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = FALSE)
options(java.parameters = c("-XX:+UseConcMarkSweepGC", "-Xmx8192m"))
#options(java.parameters = "-Xmx8g")
```

 

\newpage

```{r, echo=F, message=FALSE, warning=FALSE}
# Operational Packages
library(dplyr) # Additional Base level functionality of R
library(ggplot2) # Comprehensive Graphical Output Package
library(ggthemes) # More themes for GGPLOT2
library(Rfast) # Fast R Functions
library(bestNormalize) # Automatic Optimal Normalization
library(rlist) #list.append
library(scales) # Control over Axis formatting
library(kableExtra) # Output HTML Formatted Tables
library(reshape2) # Melt Function Correlation Matrix
library(lsr) #Correlation Matrix
library(e1071) #Skewness and other depreciated functions
library(stats)
library(lubridate) # For working with dates


library(caret) # MachineLearning package
library(doSNOW) # Parralel processing
library(rminer) #Mmetric Function'


library(tidyverse)
library(ggplot2)
library(reshape2)
library(ggthemes) #General Theme Package
library(gghighlight)
library(gridExtra) # Plot in Grids

library(xtable)
library(papeR)
library(reporttools)
```

\newpage

# EDA Assignment

## Business Problem

Maverik would like to improve sales forecasting of candy bars. The convenience stores must have accurate sales models for the inventory held at each location to generate accurate budgets and financial forecasts.

## Analytic Objective

The analytic objective is to build a model to predict the outcome variable: "Sales" using historical candy bar sales data.

## Questions Guiding EDA

1.  Where can we find the information in the data set? Do variables hold data in themselves, or is most data visible when variables are examined overtime?

2.  Will we be able to impute NA weather data using web scraping, or should they be removed?

3.  Are the missing (NA) 11K weather data points related to the same date and location, or are the values missing randomly?

4.  What is the extreme outlier 96994.40 in Sales, and what other significant outliers exist?

5.  Do we need to perform any standardization or normalization?

6.  Which variables need to be factorized? Do other data type changes need to be made?

7.  Are any of the variables strongly correlated, and do singularities and multicollinearity occur?

8.  Do we need to adjust the date format variable so that we can use it in prediction models?

9.  Where can we find the information in the data set? Do variables hold data in themselves, or is most data visible when variables are examined overtime?

\newpage

# Method

## Preliminary Analysis

The data supplied comes in two datasets; 1) Project Info Revised and 2) Products. Find the codebook in appendix A for both datasets. The Project Info Revised is our primary dataset for the project. The Products dataset is a dictionary for the variable "item_id" in the primary dataset. By combining the two data sets, we will be able to assess data validity properly. The aim is to join the two data sets by mapping Item_description values with Item_ID values and creating a new variable in the primary dataset named Item_Desc. The Products dataset will not be used after value mapping the variables.

### Data Set-up

```{r}
download.file(
  "https://github.com/Kydoimos97/CapstoneMSBA2020/raw/main/Data/CapstoneProjectInfoRevised.rds", 
  destfile = "CapstoneProjectInfoRevised.rds")

download.file(
  "https://github.com/Kydoimos97/CapstoneMSBA2020/raw/main/Data/CapstoneProjectProducts.rds", 
  destfile= "CapstoneProjectProducts.rds")

df <- readRDS("CapstoneProjectInfoRevised.rds")
products <- readRDS("CapstoneProjectProducts.rds")
```

## Setting Data Types

```{r}
# Factorization
df$Site_ID <- as.factor(df$Site_ID)
df$Location_ID <- as.factor(df$Location_ID)
df$Locale <- as.factor(df$Locale)
df$Fiscal_Period <- as.factor(df$Fiscal_Period)
df$MPDS <- as.factor(df$MPDS)
df$Project <- as.factor(df$Project)

# Numeration
df$Quantity_Sold <- as.numeric(df$Quantity_Sold)
df$SQ_Footage <- as.numeric(df$SQ_Footage)# Shows Factor like tendencies
df$Periodic_GBV <- as.numeric(df$Periodic_GBV)
df$Current_GBV <- as.numeric(df$Current_GBV)

# Re-origin of Dates at 06/22/1998

x <- min(df$Open_Date) 
df$Open_Date <- as_date(df$Open_Date, origin = x)
df$DATE <- as_date(df$DATE, origin = x)

rm(x)

# enables DATE to be used in prediction algorithms

```

A Date format in R is based on epoch time and thus easily convert to a number. The origin is reset to the earliest date present in the data set. Setting the origin allows us to work with an origin point that holds value [06/22/1998] instead of the arbitrary origin point of epoch [1/1/1970]

```{r, warning=FALSE, message=FALSE}
candy_vector <- c(products$Item_Desc)

id_vector <- c(products$Item_ID)

df$Item_desc <- df$Item_ID
df$Item_desc <- plyr::mapvalues(df$Item_desc, id_vector, candy_vector)

df$Item_ID <- as.factor(df$Item_ID)

# Reodering Data
df <- df[, c(1,2,6,17,7,3,12,13,14,10,11,8,9,15,16,4,5)]

#Rename Variables
names(df) <- tolower(make.names(names(df)))

rm(products, candy_vector, id_vector)

```

## Summary Tables

### Numeric

```{r, results="asis"}
# summarize numeric variables
tableContinuous(df[,sapply(df, is.numeric)], comment = FALSE,
                stats = c("n", "min", "q1", "median", "mean", "q3", "max", "na"),
                cap = "Summary of Numeric variables")
```

Table 1 shows that after mapping the Item_desc variables to the primary dataset, the dataset consists of 1,352,124 observations over 17 variables. The temperature variables are the only variables with Missing data where 11,799 entries are missing. Before data cleaning, the team needs to research whether the missing temperature data points can be imputed using the location data provided in the dataset.

Since only ten site_id's exist, it is possible to interpolate these sites' location by matching historical temperature data with the given temperature data; however, the problem is the cost of accessing relevant API's is over \$100 per month. Web scraping is an option yet a legal grey area primarily since this project is conducted for a corporate sponsor. Further opportunities for imputation have to be explored and will likely be based on a form of bootstrapping data based on time-series and previous years or removal of temperature data overall.

Table 1 also shows that Quantity sold and sales seem to have a long right tail. Since Sales is the target variable, this needs to be further investigated and likely transformed to ensure optimal algorithm performance.

\newpage

#### Temprature NA's

```{r}
new_df <- df[is.na(df$maxtemp),]
```

```{r, eval=F}
levels(as.factor(new_df$site_id))

levels(as.factor(new_df$date))
```

**Refer to Appendix B and C for output.** This deeper dive into the NA's show that the missing temperature data is randomly scattered across dates and sites, and we cannot point to a single factor for why the information is missing. We will need to perform further analysis during data cleaning to determine whether we will impute or remove the NA's.

\newpage

### Factorized

```{r, results="asis"}
# summarize factorized variables
tableNominal(df[c(12,13,14,15)], cap = "Summary of Factorized variables", cumsum = FALSE,
             comment = FALSE)

```

Table 2 shows that the Project variable holds limited information since only 0.02% of the data entries deviate from NONE's baseline. MPDS and sq_footage seem to not change during the data set's timeline, the same for locale. Therefore, we have to decide what variable holds the most information to eliminate multicollinearity and singularity when running algorithms.

\newpage

## Multicolinnearity

Multicollinearity harms model performance and therefore needs to be eliminated. The negative impact on model performance originates from two main points. First, multicollinearity makes coefficients extremely sensitive, virtually eliminating the ability to generalize a model. Second, multicollinearity can misrepresent the P-value, making it hard to rely on model results to readjust or retain null-hypothesizes. We will eliminate multicollinearity as the two previously discussed underlying problems would have a detrimental effect on the model outcome. In this current EDA, we will be looking at where multicollinearity exists and what engineering to consider for feature engineering.

To eliminate multicollinearity, we will follow the correlation guidelines below and remove any strong relationship of an absolute r above 0.7. (Mindrila & Balentyne).

Absolute Value of r Strength of Relationship

-   r \< 0.3 None or very weak

-   0.3 \< r \< 0.5 Weak

-   0.5 \< r \< 0.7 Moderate

-   r \> 0.7 Strong

**Note:** Na's are omitted; however, testing showed that this does not change results at all but merely allows the correlation matrix to be complete instead of showing gaps for correlations regarding temperature data.

### Correlation Matrix: Pre-Engineering

```{r, fig.cap='Pre-engineering, Correlation Matrix'}
df2 <- df
df2$open_date <- as.numeric(df2$open_date)
df2$date <- as.numeric(df2$date)

nums <- unlist(lapply(df2, is.numeric))
Integers <- na.omit(df2[ , nums]) # Remove NA's due to temp

melted_cormat <- melt(cor(Integers))

names(melted_cormat)[3] <- "Correlation"

melted_cormat <- melted_cormat %>% 
    arrange(Var1) %>%
    group_by(Var1) %>%
    filter(row_number() >= which(Var2 == Var1))

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=Correlation)) + 
  geom_tile(aes(fill=Correlation), col = "Black") + 
  theme_fivethirtyeight () +
  theme(legend.position="right",legend.direction = "vertical",
        legend.text = element_text(size=7)) +
  geom_text(label = round(melted_cormat$Correlation, digits = 2),
            color="Black", size=3, alpha=0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1, size=10),
        axis.text.y = element_text(angle=0,vjust = 0.5, hjust=1, size=10)) +
  scale_fill_gradient2_tableau("Classic Orange-White-Blue Light", limits = c(-1,1)) +
  theme(axis.title = element_text()) + 
  ylab("Feature") + 
  xlab("Feature") + 
  labs(title = "Correlation Matrix")
  

rm(Integers, melted_cormat, nums, df2)
```

As shown in Figure 1, most multicollinearity shows between the max and min temperature variables. We expected this result since min and max temp would move closely together in any direction. We need to consider if retaining one variable would provide a better model outcome since they are redundant. We can remedy this multicollinearity by making a temperature range variable by subtracting min temp from max temp. The new variable temp_diff would show when large swings in temperature occurred, which could point to weather events. We assume that the minimum temperature occurs at night outside of regular customer hours, and therefore, we will remove min temp as a variable. We will leave Maxtemp as a variable as we assume the maximum temperature occurs during the day during regular customer hours.

Next, the GBV variables are heavily correlated with each other and with sq_footage. GBV stands for gross book value. Logically, a large store would have a more considerable Gross book value. We will eliminate this correlation relationship by creating two new variables. The first variable is cgbv_sqf, which stands for current GBV per square footage. This variable shows the valuation per store and removes the information inherited for the sq_footage variable. A variable will then be made that shows the difference between the current and periodic GBV by subtracting the latter from the former. This difference again removes the inherited data from the square footage. After creating the new variables, we will remove current_gbv and periodic_gbv variables.

\newpage

### Removal and Creation of variables

```{r}
# creation of tempdiff
df$temp_diff <- as.numeric(df$maxtemp-df$mintemp)

# Creation of cgbv_sqf
df$cgbv_sqf <- as.numeric(df$current_gbv/df$sq_footage)

# creation of diff_gbv
df$diff_gbv <- df$current_gbv - df$periodic_gbv

#Days open
x <- min(df$open_date) 
df$days_open <- as.numeric(df$date-df$open_date)

rm(x)

df <- df[, c(1,2,3,4,21,6,7,19,20,10,18,12,13,14,15,16,17,8,9,11,5)]

df <- df[,-c(12,18,19,20,21)]
```

### Corelation Matrix: Post-Engineering

```{r, fig.cap='Post-engineering, Correlation Matrix'}
df2 <- df
df2$date <- as.numeric(df2$date)

nums <- unlist(lapply(df2, is.numeric))
Integers <- na.omit(df2[ , nums]) # Remove NA's due to temp

melted_cormat <- melt(cor(Integers))

names(melted_cormat)[3] <- "Correlation"

melted_cormat <- melted_cormat %>% 
    arrange(Var1) %>%
    group_by(Var1) %>%
    filter(row_number() >= which(Var2 == Var1))

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=Correlation)) + 
  geom_tile(aes(fill=Correlation), col = "Black") + 
  theme_fivethirtyeight () +
  theme(legend.position="right",legend.direction = "vertical",
        legend.text = element_text(size=7)) +
  geom_text(label = round(melted_cormat$Correlation, digits = 2),
            color="Black", size=3, alpha=0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1, size=10),
        axis.text.y = element_text(angle=0,vjust = 0.5, hjust=1, size=10)) +
  scale_fill_gradient2_tableau("Classic Orange-White-Blue Light", limits = c(-1,1)) +
  theme(axis.title = element_text()) + 
  ylab("Feature") + 
  xlab("Feature") + 
  labs(title = "Correlation Matrix")
  

rm(Integers, melted_cormat, nums, df2)
```

Figure 2 shows the deployed strategies eliminated all occurrences of moderate to strong multicollinearity. Only weak multicollinearity remains, which will allow the algorithms to perform most optimally.

\newpage

## Outlier Detection and Relationships

**Note:** Geom_Point plots are limited to 1% of the data due to performance issues with PDF's. While using only 1% of the data removes most data points, it did not significantly affect the way plots look since most data points overlap. We chose 1% since rendering the plots below in a PDF becomes near impossible due to PDF's not saving an image but every data point as an object. When knitting finished, opening the document in any PDF viewer would crash due to the sheer amount of data in these plots alone. We are currently investigating rendering to PNG's and implementing them in PDF's to circumvent this problem.

**Note:** For visualization purposes, the sales data axes have been limited to 0 to 150. This number shows most of the information and omits outliers that lay on the right tail of sales.

### Outlier Detection and Relationships:Numerical Variables seperated by Site_ID

```{r, fig.height = 9, fig.width = 7, fig.cap='Relationships between Target and Numeric Variables seperated by Site_ID', warning=FALSE, message=FALSE}
num_list <- colnames(df[sapply(df, is.numeric)])
num_list <- num_list[-c(4,5,6,7)]
plot_list <- list(1)

counter = 0

resample <- caret::createDataPartition(y=df$sales, p = 0.01, list=FALSE) 
#This is due to a limitation on PDF's
df2 <-df[resample,]

mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

for (i in num_list) {
  
  counter = counter+1
  x = ggplot(data=na.omit(df2[df2$sales < 100,]), aes_string(x = "sales", y = i, color="site_id")) +
      geom_point(position = "identity", stat = "identity", alpha = .35, size=1, stroke=.05) + 
      theme_fivethirtyeight() + 
      scale_color_manual(values = mycolors) +
      scale_y_continuous(labels = comma) + scale_x_continuous(labels = dollar) + coord_flip() + 
      ggtitle(paste0("sales vs ", as.character(i), sep= " ")) + 
      theme(plot.title = element_text(size=10), legend.position = "right", legend.direction = "vertical") + 
      guides(color = guide_legend(override.aes = list(size=2.5, alpha = 1) ,title = "Legend", title.position = "top"))
  
  
  assign(paste0("plt",i), x)
  plot_list <- rlist::list.append(plot_list, x)
}

plot_list <- plot_list[-1]

nCol <- floor(sqrt(length(plot_list)))

grid.arrange(grobs=plot_list, widths = c(1,1), ncol=2, layout_matrix =  rbind(c(1,1),
                                                                                 c(2,2), 
                                                                                 c(3,3)), 
             top = "Relationships between Target and Numeric Variables seperated by Site_ID")

for (i in num_list) {
  rm(list = paste0("plt",i))
}

rm(plot_list, counter, num_list, x, i, nCol, df2, mycolors, resample)
```

Figure 3 shows the relationships between the target variable sales and several numeric variables. All of these numeric variables are location bound and therefore Colored by Site_ID. When looking at days_open, there is a clear correlation with Site_ID. This relationship is logical due to each location opening on a different day. There is no clear relationship between sales and days_open.

Sales versus current gross book value per square foot show current book value per site does not seem to change at all in the data set. There are no other clear relationships between sales and current gross book value per square foot.

The Sales versus the difference in gross book value shows two cluster points. The larger cluster is around 0, and the smaller group is centered around 1,200,000.

\newpage

### Outlier Detection and Relationships:Other Numerical Variables

```{r, fig.height = 9, fig.width = 7, fig.cap='Relationships between Target and Numeric Variables', warning=FALSE, message=FALSE}
num_list <- colnames(df[sapply(df, is.numeric)])
num_list <- num_list[-c(1,2,3,7)]
plot_list <- list(1)

counter = 0
colcounter = 0

resample <- caret::createDataPartition(y=df$sales, p = 0.01, list=FALSE)
df2 <-df[resample,]

mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

for (i in num_list) {
  
  counter = counter+1
  colcounter = counter*3
  x = ggplot(data=na.omit(df2[df2$sales < 150,]), aes_string(x = "sales", y = i)) +
      geom_point(position = "identity", stat = "identity", alpha = .75, color="grey", size=0.75)  + 
      scale_y_continuous(labels = comma) + scale_x_continuous(labels= dollar) + 
      geom_smooth(method = lm, se=TRUE, color=mycolors[colcounter], size=1, fullrange = TRUE) + 
      ggtitle(paste0("sales vs ", as.character(i), sep= " ")) +
      theme_fivethirtyeight() + theme(plot.title = element_text(size=10)) 
  
  
  assign(paste0("plt",i), x)
  plot_list <- rlist::list.append(plot_list, x)
}

plot_list <- plot_list[-1]

nCol <- floor(sqrt(length(plot_list)))

grid.arrange(grobs=plot_list, widths = c(1,1), ncol=2, layout_matrix =  rbind(c(1,1),
                                                                                 c(2,2), 
                                                                                 c(3,3)), 
             top = "Relationships between Target and Numeric Variables")


for (i in num_list) {
  rm(list = paste0("plt",i))
}

rm(plot_list, counter, num_list, x, i, nCol, df2, resample, mycolors, colcounter)
```

Figure 4 shows a slight positive relationship between max temp and sales. The graph shows more candy bar sales with higher temperatures across locations. However, the new variable, temperature difference, shows a weak relationship with sales. Lastly, sales and quantity sold show a strong positive relationship as expected; more quantity sold results in increased sales.

```{r, results='asis', warning=FALSE, message=FALSE}
table <- top_n(df[c(1,2,4,15,16)], 5)

xtable(table, caption="Top 5 Sales values in the data set", comment=FALSE)
```

In reviewing the data, there appear to be some extreme outliers in Sales. The median Sales value is just \$2.79, which would generally have a sales volume of one or two per candy bar. The interquartile range (IQR) from the 25th percentile to the 75th percentile spans just \$2.98 from \$1.79 to \$4.77. However, there was an extreme outlier of **\$96,994.40** in sales on 3/14/2020 from the sale of 2,000 Tootsie Rolls (Item_ID -12066). We are concerned this might be inaccurate data since the product info indicates they sell for just \$0.10 each. Based on selling 2,000 Tootsie Rolls at \$0.10 each would only come to \$200.00, which is certainly quite a bit short of the \$96,994.40 indicated in the data. The outlier is likely incorrect information and would be worth confirming with Site ID 380 since they would likely remember such an extreme sale.

\

When looking at quantity sold, the IQR spans just one candy bar from just 1 to 2 candy bars. However, on 3/28/20, Site_ID 380 (Suburban/Residential) sold 10,048 of Item_ID -17,625 (I\*HER REESES EGG 1.2z LTO) at \$1.00 each for total sales of \$10,048.

\

There are other significant outliers as well, but these two were the most extreme. We will continue looking into the data and confirming with Maverik to see if these data points are correct.

\newpage

### Outlier Detection and Relationships: Categorical Variables

In this section, boxplots are created with accompanying tables to precisely read the data. These variables will need to be evaluated later by aggregating all sales on a day and rerunning the code below to find differences between days instead of products and see if factor levels show different key_metrics.

#### Categorical Variables: Locale

Figure 5 and Table 4 show only slight differences between different factor levels of sales and locale. Each factor level's sample size is decently distributed, with only Suburban/residential only deviating a lot from the average sample size. With industrial having a higher sales median of 3.18 over 2.79 and lower for the rest of the factor levels, it warrants more research to determine what products warrant these extra sales.

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and Locale'}
i = "locale"
o <- c(summary(df$locale))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() + 
  ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + 
  theme_fivethirtyeight() + theme(axis.text.x = element_text(angle =0)) + 
  scale_y_discrete(guide = guide_axis(n.dodge=2))+
  theme(plot.title = element_text(size = 10, face = "bold")) + 
  scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
```

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis"}
y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Sales and Locale between different factor levels"), 
      comment = FALSE)

```

\newpage

#### Categorical Variables: MPDS

Figure 6 and Table 5 show no clear relationship between the number of mpds and sales. The lack of a relationship indicates that sales of individual items are more site-specific and are not based on the number of pumps present at the gas station.

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and MPDS'}
i = "mpds"
o <- c(summary(df$mpds))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() +   ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + 
  theme_fivethirtyeight() + 
  theme(axis.text.x = element_text(angle =0)) + 
  scale_y_discrete(guide = guide_axis(n.dodge=1))+
  theme(plot.title = element_text(size = 10, face = "bold")) + 
  scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
```

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis"}
y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Sales and MPDS between different factor levels"), comment = FALSE)

```

\newpage

#### Categorical Variables: Project

Table 6 and figure 7 show no clear relationship between factor levels of project. Both factor levels apart from NONE have a tiny sample size, and generalization should happen carefully. Program enhancement seems to result in lower median sales, where enhancement likely means that existing parts of the building have had to close, as opposed to none and interfering with a location's ability to sell products.

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and Project'}
i = "project"
o <- c(summary(df$project))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() + 
  ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + 
  theme_fivethirtyeight() + 
  theme(axis.text.x = element_text(angle =0)) + 
  scale_y_discrete(guide = guide_axis(n.dodge=2)) +
  theme(plot.title = element_text(size = 10, face = "bold")) + 
  scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
```

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis"}
y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Sales and Project between different factor levels"),
             comment = FALSE)

  
rm(i, o, y, x)
```

\newpage

####Outlier Detection and Relationships: Time Series of Sales per Site_ID aggregated per day

```{r}
sum_sales <- aggregate(df$sales, by=list(df$date, df$site_id), FUN=sum)
names(sum_sales) <- c("Date", "Site_ID", "Sum_of_Sales")
```

```{r, fig.height = 9, fig.width = 7, fig.cap='Sales over time', warning=FALSE, message=FALSE}
mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

ggplot(sum_sales, aes(x=Date, y=Sum_of_Sales, color=Site_ID)) +
  facet_wrap(~Site_ID, scale="free_y", nrow = 5, ncol = 2) +
  geom_point(position = "identity", stat = "identity", alpha = .8, color="grey", size=0.75) +
  geom_smooth(se=F) +
  theme_fivethirtyeight() +
  scale_color_manual(values = mycolors) +
  scale_y_continuous(labels = scales::dollar, limits = c(0,1500)) +
  theme(legend.position = ("none"), strip.text = element_text(size=15), plot.title = element_text(size=22),axis.title.x = element_text(size = 15), axis.title.y =     element_text(size = 15)) +
  labs(x = "Site_ID", y = "Occupation Growth in percentage", title = "Total Sales per Site_ID ")

```

\newpage

```{r, results='asis'}
counter = 0

list_siteid <- NULL
list_mpds <- NULL
df2 <- NULL

for (i in c(levels(df$site_id))) {
   counter = counter+1
   list_siteid[counter] <- i
   df2 <- df %>% filter(site_id == i)
   list_mpds[counter] <- mean(as.numeric(as.character(df2$mpds)))
  
}

#Latex does not work nice with _ so site_id Site_ID due to results= asis
table2 <-data.frame(SiteID = list_siteid, MPDS = list_mpds)


```

```{r, results='asis', warning=FALSE, message=FALSE}

print.xtable(xtable(table2, caption = "Amount of Pumps per SiteID"), comment=FALSE)

rm(table2, list_mpds, list_siteid, i, counter, df2)
```

Figure 8 shows the aggregate sum of sales per day grouped by site_id, or location. There is a clear difference between various sites in the amount of aggregated sales per day. Every location shows differences in seasonality, e.g., 380 vs. 280, and growth, e.g., 280 vs. 517. Table 7 shows the site_id and the matching amount of pumps. There is no apparent difference in the number of pumps and sales in a given day. Seasonality also does not appear to depend on the number of pumps available at the get station, and neither does growth.

\newpage

# Results

Through the exploratory analysis we performed on our data, we have discovered the following:

1.  We cannot currently predict any values correctly without running a more complex analysis. After trying to use a standard GLM algorithm, we found that it could not accurately predict any Sales value. With that in mind, we will be researching the best method for performing other forms of analysis and which platform is best for this use case (i.e., Python or R). Additionally, we will explore different algorithms that can better handle the data provided.

2.  After mapping the supplementary data set to the original, we found that the temperature variable contained 11,799 NA values. We have a few thoughts on how to go about this but want to do some more research to find best practices. Our insights are to possibly use historical temperature data by site_id to fill in the NA values.

3.  Multicollinearity is present in this data set and will need to be resolved for our final analysis. It exists between the max temp and the min temp and the Gross Book Value, and the Square Footage. For the temperature variables, our thoughts are to exclude Min Temp since that generally occurs during the middle of the night, where Max Temp is a more accurate representation of the regular hours of operation. For GBV and sq_footage, our thoughts are to create two separate variables to help standardize the data.

4.  This data set deals with Singularities, where two variables hold precisely the same or close to the same data. Especially Site_ID, Days_open, and current GBV show singularities as they all contain the same information. We need to thoroughly clean this data set and compare variables to show precisely what information it holds and where we can find information that we can predict so we can focus on the most critical information.

5.  Aggregating sales data and displaying it overtime shows seasonality. It can be interesting to dive deeper into an aggregated data sets and learn more about the underlying causes of daily sales before we apply this knowledge to feature engineer the data set to work with candy bar specific sales. Additionally, trying to discover trends by creating visualizations of variables over time will allow us to see where predictive power lies within the data set. Finding trends, however, requires a large number of visualizations, and we did not have the time to go over everything yet by the time of handing in this paper.

Currently, we cannot derive much insight into the overall predictability of our variables using a general linear model. However, we could find issues within the data that will help us be more prepared for when we perform more complex analyses. This EDA has helped us understand the different approaches we need to take both in pre-processing and our time-series method. As it stands, dealing with multicollinearity and the NA values and focussing on trends that show over time will help us predict our model.

# Team Member Contribution

Sam, Willem, and Kelsey produced an initial EDA working together. Tim also created an initial EDA, which was compared to the group EDA. Tim initially worked due to scheduling issues. We merged the initial elements of both documents. Willem then started a master document by coding the full EDA and writing an initial write-up. Kelsey, Willem, Sam, and Tim then added more in-depth written analysis. Additionally, Kelsey, Sam, and Willem proofread and provided final edits to the document. For collaboration, the whole team used GitHub to make final edits and adjustments to the EDA.

\newpage

# Bibilography

Mindrila, D., & Balentyne, P. (n.d.). *Scatterplots and Correlation*. Retrieved November 20, 2020, from <https://www.westga.edu/academics/research/vrc/assets/docs/scatterplots_and_correlation_notes.pdf>

\newpage

# Appendix

\newpage

## Appendix: A

**CodeBook: Main Dataset**

-   Variable_Name Data_Type Description

-   SITE_ID INT = Unique Site Identifier (Maverik Store ID)

-   LOCATION_ID INT = Location Identifier, 1:1 with SITE_ID

-   SQ_FOOTAGE INT = Square Footage of Store (Indoor Footprint)

-   LOCALE CHR = Geospatial Descriptor

-   MPDS INT = Count of Fueling Positions

-   DATE DATE = Day Date

-   DAY_OF_WEEK CHR = Day of Week Descriptor

-   HOLIDAY CHR = Holiday Description, If Applicable

-   HCA_GBV NUM = Gross Book Value of Site

-   MAX_TEMP NUM = Daily Maximum Temperature

-   MIN_TEMP NUM = Daily Minimum Temperature

-   PROJECT CHR = If Day's Sales Was Impacted by a Project

-   ITEM_ID INT = Item Identifier

-   SALES INT = Sum of Sales

-   QUANTITY_SOLD = INT Sum of Units

**CodeBook: Products Mapping Subset**

-   Item_ID = Item Identifier

-   Item_Desc = Item Description per Identifier

-   Department_Decs = Product department description

\newpage

## Appendix: B

Site_ID's with missing Temperature Values

\begin{itemize}
```{r, results='asis', echo=FALSE}

cat(paste("\\item", c(levels(as.factor(new_df$site_id))), sep = "\n"))

```
\end{itemize}

## Appendix: C

Dates with missing Temperature Values

\begin{itemize}
```{r, results='asis', echo=FALSE}

cat(paste("\\item", c(levels(as.factor(new_df$date))), sep = "\n"))

```
\end{itemize}

\newpage
