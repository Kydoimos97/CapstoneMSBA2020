---
title: "EDA Capstone"
author: "Willem van der Schans"
date: "2/3/2021"
output:
  pdf_document:
    fig_height: 4
    fig_width: 8
    highlight: kate
    toc: no
    toc_depth: 4
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: kate
    number_sections: no
    theme: paper
    toc: yes
    toc_float: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = FALSE)
options(java.parameters = c("-XX:+UseConcMarkSweepGC", "-Xmx8192m"))
#options(java.parameters = "-Xmx8g")
```

```{r, echo=F, message=FALSE, warning=FALSE}
# Operational Packages
library(dplyr) # Additional Base level functionality of R
library(ggplot2) # Comprehensive Graphical Output Package
library(ggthemes) # More themes for GGPLOT2
library(Rfast) # Fast R Functions
library(bestNormalize) # Automatic Optimal Normalization
library(rlist) #list.append
library(scales) # Control over Axis formatting
library(kableExtra) # Output HTML Formatted Tables
library(reshape2) # Melt Function Correlation Matrix
library(lsr) #Correlation Matrix
library(e1071) #Skewness and other depreciated functions
library(stats)
library(lubridate) # For working with dates
library(gridExtra) # Plot in Grids

library(caret) # MachineLearning package
library(doSNOW) # Parralel processing
library(rminer) #Mmetric Function'


library(tidyverse)
library(ggplot2)
library(reshape2)
library(ggthemes) #General Theme Package
library(gghighlight)

library(xtable)
library(papeR)
library(reporttools)


```

**Note to Team: We need to focus on everything above feature engineering and machine learning, the way we are predicting in this document makes no sense as we cannot predict anything right now. We need to transmute the data and see if we can find information to use through other statistical means.**

# EDA Assignment

## Business Problem

Maverik would like to improve sales forecasting of candy bars.  The convenience stores must have accurate sales models for the inventory held at each location to generate accurate budgets and financial forecasts.  

## Analytic Objective

The analytic objective is to build a model to predict the outcome variable “Sales” using historical candy bar sales data.

## Questions Guiding EDA

Will we be able to impute NA weather data using web scraping?
Are the missing (NA) 11K weather data points related to the same date, location, etc, or are they isolated incidents?
Should we remove the 11K NAs?
What is the extreme outlier 96994.40 in Sales?
Do we need to remove the outliers?
Do we need to perform any standardization or normalization?
Do we need to perform log transformations?
Which variables need to be factorized?
Do other data type changes need to be made?
Are any of the variables strongly correlated? Multicollinearity? 
Do we need to adjust the Open_date variable so that it can be used in prediction models?


\newpage

## CodeBook: Main Dataset

-   Variable_Name Data_Type Description 

-   SITE_ID INT = Unique Site Identifier (Maverik Store ID) 

-   LOCATION_ID INT = Location Identifier, 1:1 with SITE_ID 

-   SQ_FOOTAGE INT = Square Footage of Store (Indoor Footprint) 

-   LOCALE CHR = Geospatial Descriptor 

-   MPDS INT = Count of Fueling Positions 

-   DATE DATE = Day Date 

-   DAY_OF_WEEK CHR = Day of Week Descriptor 

-   HOLIDAY CHR = Holiday Description, If Applicable 

-   HCA_GBV NUM = Gross Book Value of Site 

-   MAX_TEMP NUM = Daily Maximum Temperature 

-   MIN_TEMP NUM = Daily Minimum Temperature 

-   PROJECT CHR = If Day's Sales Was Impacted by a Project 

-   ITEM_ID INT = Item Identifier 

-   SALES INT = Sum of Sales 

-   QUANTITY_SOLD = INT Sum of Units

### CodeBook: Products Mapping Subset

-   Item_ID = Item Identifier 

-   Item_Desc = Item Description per Identifier

-   Department_Decs = Product department description

\newpage

# Method

## Preliminary Analysis

The data supplied comes in two datasets; 1) Project Info Revised and 2) Products.  The Project Info Revised is our primary dataset for the project.   The Products dataset is a dictionary for the variable “item_id” in the primary dataset.  By combining the two data sets, we will be able to assess data validity properly. The aim is to join the two data sets by mapping Item_description values with Item_ID values and creating a new variable in the primary dataset named Item_Desc. The Products dataset will not be used after value mapping the variables.

```{r}
download.file(
  "https://github.com/Kydoimos97/CapstoneMSBA2020/raw/main/Data/CapstoneProjectInfoRevised.rds", 
  destfile = "CapstoneProjectInfoRevised.rds")

download.file(
  "https://github.com/Kydoimos97/CapstoneMSBA2020/raw/main/Data/CapstoneProjectProducts.rds", 
  destfile= "CapstoneProjectProducts.rds")

df <- readRDS("CapstoneProjectInfoRevised.rds")
products <- readRDS("CapstoneProjectProducts.rds")

```{r}
head(df)
head(products) # shows we looked at the data types before factorization and numeration

```

```{r}
# Factorization
df$Site_ID <- as.factor(df$Site_ID)
df$Location_ID <- as.factor(df$Location_ID)
df$Locale <- as.factor(df$Locale)
df$Fiscal_Period <- as.factor(df$Fiscal_Period)
df$MPDS <- as.factor(df$MPDS)
df$Project <- as.factor(df$Project)

# Numeration
df$Quantity_Sold <- as.numeric(df$Quantity_Sold)
df$SQ_Footage <- as.numeric(df$SQ_Footage)# Shows Factor like tendencies
df$Periodic_GBV <- as.numeric(df$Periodic_GBV)
df$Current_GBV <- as.numeric(df$Current_GBV)

# Re-origin of Dates  at 1/1/1980

x <- min(df$Open_Date) 
df$Open_Date <- as_date(df$Open_Date, origin = x)
df$DATE <- as_date(df$DATE, origin = x)

rm(x)

# enables DATE to be used in prediction algorithms

```

```{r, warning=FALSE, message=FALSE}
candy_vector <- c(products$Item_Desc)

id_vector <- c(products$Item_ID)

df$Item_desc <- df$Item_ID
df$Item_desc <- plyr::mapvalues(df$Item_desc, id_vector, candy_vector)

df$Item_ID <- as.factor(df$Item_ID)

# Reodering Data
df <- df[, c(1,2,6,17,7,3,12,13,14,10,11,8,9,15,16,4,5)]

#Rename Variables
names(df) <- tolower(make.names(names(df)))

rm(products, candy_vector, id_vector)
```

```{r, results="asis"}
# summarize numeric variables
tableContinuous(df[,sapply(df, is.numeric)], 
                stats = c("n", "min", "q1", "median", "mean", "q3", "max", "na"),
                cap = "Summary of Numeric variables")
```

Table 1 shows that after mapping the Item_desc variables to the main dataset the dataset consists of 1,352,124 observations over 17 variables. The temperature variables are the only variables with Missing data where 11,799 entries are missing.  Before data cleaning, the team needs to research whether the missing temperature data points can be imputed using the location data provided in the dataset. 

**How do we know these are Utah based sites? Maverik has locations across 11 states?  I don’t think the location information provided in the dataset would give us enough information to be able to impute NAs.  We need to consider dropping the 11K or the temp variables.  Try models both ways? Site_Id and Location_Id provide the same information** 

  Since only 10 site_id's exist it is possible to interpolate the location of these sites by matching historical temprature data with the given temprature data, and the knowledge that the area of operations lies around utah severely limiting the scope, however the problem is the cost of accessing relevant API's is over \$100 dollar per month. Webscraping is an option however a legal grey area especially since this project is conducted for a corperate sponsor. Further options for imputation have to be explored and will likely be based on a form of bootstrapping data based on timeseries and previous years or removal of temprature data overall.


\newpage

```{r, results="asis"}
# summarize non-numeric variables
tableNominal(df[c(12,13,14)], cap = "Summary of Numeric variables", cumsum = FALSE)
```

**The table output I see has sq_footage, locale, and mpds no quantity sold or sales**

Table 2 shows that Quantity sold and sales seem to have extremely long right tails. Since Sales is the target variable this needs to be further investigated and looked at to ensure optimal algorithm performance. The Project variable holds limited information since only 0.02% of the data entries deviate from the baseline of NONE.

\newpage

## Multicolinnearity

Multicolinnearity has a negative effect on model performance and therefore needs to be elimited. The negative impact on model performance originates from two main points. First, multicollinearity makes coefficients extremely sensitive, effectively eliminating the ability to generalize a model. Second, multicollinearity can misrepresent the P-value, making it hard to rely on model results to readjust or retain null-hypothesises. We will eliminate multicollinearity as the two underlying problems would have a detrimental effect on the model outcome's efficacy. In this current EDA, we will be looking at where multicollinearity exists and what engineering to consider for feature engineering.

To eliminate multicollinearity, we will follow the correlation guidelines below and remove any strong relationship of an absolute r above 0.7. (Mindrila & Balentyne).


**Absolute Value of r Strength of Relationship**

r \< 0.3 None or very weak

0.3 \< r \< 0.5 Weak

0.5 \< r \< 0.7 Moderate

r \> 0.7 Strong

Note: that Na's are omitted however testing showed that this does not change results at all, but merely allows the correlation matrix to be complete instead of showing gaps for correlations regarding temprature data.

### Corelation Matrix: Pre-Engineering

```{r, fig.cap='Pre-engineering, Correlation Matrix'}
df2 <- df
df2$open_date <- as.numeric(df2$open_date)
df2$date <- as.numeric(df2$date)

nums <- unlist(lapply(df2, is.numeric))
Integers <- na.omit(df2[ , nums]) # Remove NA's due to temp

melted_cormat <- melt(cor(Integers))

names(melted_cormat)[3] <- "Correlation"

melted_cormat <- melted_cormat %>% 
    arrange(Var1) %>%
    group_by(Var1) %>%
    filter(row_number() >= which(Var2 == Var1))

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=Correlation)) + 
  geom_tile(aes(fill=Correlation), col = "Black") + 
  theme_fivethirtyeight () +
  theme(legend.position="right",legend.direction = "vertical",
        legend.text = element_text(size=7)) +
  geom_text(label = round(melted_cormat$Correlation, digits = 2),
            color="Black", size=3, alpha=0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1, size=10),
        axis.text.y = element_text(angle=0,vjust = 0.5, hjust=1, size=10)) +
  scale_fill_gradient2_tableau("Classic Orange-White-Blue Light", limits = c(-1,1)) +
  theme(axis.title = element_text()) + 
  ylab("Feature") + 
  xlab("Feature") + 
  labs(title = "Correlation Matrix")
  

rm(Integers, melted_cormat, nums, df2)
```

As shown in Figure 1, most multicollinearity shows between the max and min temperature variables. We expected this result since min and max temp would move closely together in any direction.   We need to consider if retaining one variable would provide a better model outcome since they are redundant. We can remedy this multicollinearity by making a temperature range variable by subtracting mintemp from maxtemp.  The new variable temp_diff would show when large swings in temperature occurred, which could point to weather events. We assume that the minimum temperature occurs at night outside of regular customer hours, and therefore, we will remove mintemp as a variable. We will leave Maxtemp as a variable as we assume the maximum temperature occurs during the day during regular customer hours.  

Next, the GBV variables are heavily correlated with each other and with sq_footage. GBV stands for gross book value. Logically, a large store would have a larger Gross book value. We will eliminate this correlation relationship by creating two new variables. The first variable is cgbv_sqf, which stands for current GBV per square footage. This variable shows the valuation per store and removes the information inherited for the sq_footage variable. A variable will then be made that shows the difference between the current and periodic GBV by subtracting the latter from the former. This difference again removes the inherited data from the square footage. After creating the new variables, we will remove current_gbv and periodic_gbv variables.

### Removal and Creation of variables

```{r}
# creation of tempdiff
df$temp_diff <- as.numeric(df$maxtemp-df$mintemp)

# Creation of cgbv_sqf
df$cgbv_sqf <- as.numeric(df$current_gbv/df$sq_footage)

# creation of diff_gbv
df$diff_gbv <- df$current_gbv - df$periodic_gbv

#Days open
x <- min(df$open_date) 
df$days_open <- as.numeric(df$date-df$open_date)

rm(x)

df <- df[, c(1,2,3,4,21,6,7,19,20,10,18,12,13,14,15,16,17,8,9,11,5)]

df <- df[,-c(12,18,19,20,21)]
```

### Corelation Matrix: Post-Engineering

```{r, fig.cap='Post-engineering, Correlation Matrix'}
df2 <- df
df2$date <- as.numeric(df2$date)

nums <- unlist(lapply(df2, is.numeric))
Integers <- na.omit(df2[ , nums]) # Remove NA's due to temp

melted_cormat <- melt(cor(Integers))

names(melted_cormat)[3] <- "Correlation"

melted_cormat <- melted_cormat %>% 
    arrange(Var1) %>%
    group_by(Var1) %>%
    filter(row_number() >= which(Var2 == Var1))

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=Correlation)) + 
  geom_tile(aes(fill=Correlation), col = "Black") + 
  theme_fivethirtyeight () +
  theme(legend.position="right",legend.direction = "vertical",
        legend.text = element_text(size=7)) +
  geom_text(label = round(melted_cormat$Correlation, digits = 2),
            color="Black", size=3, alpha=0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1, size=10),
        axis.text.y = element_text(angle=0,vjust = 0.5, hjust=1, size=10)) +
  scale_fill_gradient2_tableau("Classic Orange-White-Blue Light", limits = c(-1,1)) +
  theme(axis.title = element_text()) + 
  ylab("Feature") + 
  xlab("Feature") + 
  labs(title = "Correlation Matrix")
  

rm(Integers, melted_cormat, nums, df2)
```
Figure 2 shows the deployed strategies elimited all occurences of moderate to strong multicollinearity. Only weak multicollinearity remains which will allow the algoritmhs to perform most optimally.

\newpage

## Outlier Detection and Relationships

### Outlier Detection and Relationships:Numerical Variables seperated by Site_ID

```{r, fig.height = 9, fig.width = 7, fig.cap='Relationships between Target and Numeric Variables seperated by Site_ID', warning=FALSE, message=FALSE}
num_list <- colnames(df[sapply(df, is.numeric)])
num_list <- num_list[-c(4,5,6,7)]
plot_list <- list(1)

counter = 0

resample <- caret::createDataPartition(y=df$sales, p = 0.01, list=FALSE) #This is due to a limitation on PDF's
df2 <-df[resample,]

mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

for (i in num_list) {
  
  counter = counter+1
  x = ggplot(data=na.omit(df2[df2$sales < 100,]), aes_string(x = "sales", y = i, color="site_id")) +
      geom_point(position = "identity", stat = "identity", alpha = .35, size=1, stroke=.05) + 
      theme_fivethirtyeight() + 
      scale_color_manual(values = mycolors) +
      scale_y_continuous(labels = comma) + scale_x_continuous(labels = dollar) + coord_flip() + 
      ggtitle(paste0("sales vs ", as.character(i), sep= " ")) + theme(plot.title = element_text(size=10), legend.position = "right", legend.direction = "vertical") + 
      guides(color = guide_legend(override.aes = list(size=2.5, alpha = 1) ,title = "Legend", title.position = "top"))
  
  
  assign(paste0("plt",i), x)
  plot_list <- rlist::list.append(plot_list, x)
}

plot_list <- plot_list[-1]

nCol <- floor(sqrt(length(plot_list)))

grid.arrange(grobs=plot_list, widths = c(1,1), ncol=2, layout_matrix =  rbind(c(1,1),
                                                                                 c(2,2), 
                                                                                 c(3,3)), 
             top = "Relationships between Target and Numeric Variables seperated by Site_ID")

for (i in num_list) {
  rm(list = paste0("plt",i))
}

rm(plot_list, counter, num_list, x, i, nCol, df2, mycolors, resample)
```

Figure 3 shows the relationships between the target variable sales and a number of numeric variables. All of these numeric variables are location bound and therefore Colored by Site_ID. When looking at days_open, there is a clear correlation with Site_ID. This relationship is logical due to each location opening on a different day. There is no clear relationship between sales and days_open.

Sales versus current gross book value per square foot show current book value per site does not seem to change at all in the data set. There are no other clear relationships between sales and current gross book value per square foot

The Sales versus the difference in gross book value shows two cluster points.  The larger cluster is around 0, and the smaller group is centered around 1,200,000.

\newpage

### Outlier Detection and Relationships:Other Numerical Variables

```{r, fig.height = 9, fig.width = 7, fig.cap='Relationships between Target and Numeric Variables'}
num_list <- colnames(df[sapply(df, is.numeric)])
num_list <- num_list[-c(1,2,3,7)]
plot_list <- list(1)

counter = 0
colcounter = 0

resample <- caret::createDataPartition(y=df$sales, p = 0.01, list=FALSE)
df2 <-df[resample,]

mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

for (i in num_list) {
  
  counter = counter+1
  colcounter = counter*3
  x = ggplot(data=na.omit(df2[df2$sales < 150,]), aes_string(x = "sales", y = i)) +
      geom_point(position = "identity", stat = "identity", alpha = .75, color="grey", size=0.75)  + scale_y_continuous(labels = comma) + scale_x_continuous(labels= dollar) + 
      geom_smooth(method = lm, se=TRUE, color=mycolors[colcounter], size=1, fullrange = TRUE) + ggtitle(paste0("sales vs ", as.character(i), sep= " ")) +
      theme_fivethirtyeight() + theme(plot.title = element_text(size=10)) 
  
  
  assign(paste0("plt",i), x)
  plot_list <- rlist::list.append(plot_list, x)
}

plot_list <- plot_list[-1]

nCol <- floor(sqrt(length(plot_list)))

grid.arrange(grobs=plot_list, widths = c(1,1), ncol=2, layout_matrix =  rbind(c(1,1),
                                                                                 c(2,2), 
                                                                                 c(3,3)), 
             top = "Relationships between Target and Numeric Variables")


for (i in num_list) {
  rm(list = paste0("plt",i))
}

rm(plot_list, counter, num_list, x, i, nCol, df2, resample, mycolors, colcounter)
```
Figure 4 shows a slight positive relationship between maxtemp and sales. The graph shows more candy bar sales with higher temperatures across locations. However, the new variable, temperature difference, shows a weak relationship with sales. Lastly, sales and quantity sold show a strong positive relationship as expected; more quantity sold results in increased sales.

```{r, results='asis'}
table <- top_n(df[c(1,2,4,15,16)], 5)

xtable(table, caption="Top 5 Sales values in the data set")
```
**Table 3 Text**

\newpage

### Outlier Detection and Relationships: Categorical Variables

In this section, boxplot are created with accompanying tables to precisely read the data. These variables will need to be evaluated later by aggregating all sales on a day and rerunning the code below to find differences between days instead of products and see if factor levels show different key_metrics.

#### Categorical Variables: Locale

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and Locale'}
i = "locale"
o <- c(summary(df$locale))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() +   ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + theme_fivethirtyeight() + theme(axis.text.x = element_text(angle =0)) + scale_y_discrete(guide = guide_axis(n.dodge=2))+
  theme(plot.title = element_text(size = 10, face = "bold")) + scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
  
  y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Sales and Locale between different factor levels"), comment = FALSE)

```

Figure 5 and Table 4 show only slight differences between different factor levels of sales and locale. Each factor level's sample size is decently distributed, with only Suburban/residential only deviating a lot from the average sample size. With industrial having a higher sales median of 3.18 over 2.79 and lower for the rest of the factor levels, it warrants more research to determine what products warrant these extra sales.

\newpage

#### Categorical Variables: MPDS

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and MPDS'}
i = "mpds"
o <- c(summary(df$mpds))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() +   ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + theme_fivethirtyeight() + theme(axis.text.x = element_text(angle =0)) + scale_y_discrete(guide = guide_axis(n.dodge=1))+
  theme(plot.title = element_text(size = 10, face = "bold")) + scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
  
  y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Sales and MPDS between different factor levels"), comment = FALSE)

```

Figure 6 and Table 5 show no clear relationship can be found between the amount of mpds and sales. The lack of a relationship shows that sales of indiviual items are more site specific and are not based on the amount of pumps present at the gas station. 

\newpage

#### Categorical Variables: Project

```{r, eval=TRUE, fig.height =3, fig.width =5, fig.align = "center", results="asis", fig.cap='Relationship between Sales and Project'}
i = "project"
o <- c(summary(df$project))

  x = ggplot(data=df, aes_string(x = "sales", y = paste0("`",as.character(i),"`", sep=""))) +
  geom_boxplot() +   ggtitle(paste0("Relationship of sales and ", as.character(i), sep= " ")) +
  coord_flip() + theme_fivethirtyeight() + theme(axis.text.x = element_text(angle =0)) + scale_y_discrete(guide = guide_axis(n.dodge=2))+
  theme(plot.title = element_text(size = 10, face = "bold")) + scale_x_continuous(labels = dollar, limits = c(0,15))
  print(x)
  
  y <- as.data.frame(ggplot_build(x)$data)
  y <- y[,c(1,2,3,4,5,6)]
  y$xmax <- y$xmax-y$xmin
  y$xmin <- levels(df[,which( colnames(df)==i)])
  y$outliers <- ifelse(
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) != 0, 
    lengths(regmatches(as.character(y$outliers), gregexpr(",", as.character(y$outliers)))) + 1,
    ifelse(regmatches(as.character(y$outliers), gregexpr("[0-9]", as.character(y$outliers))) == "0", 
    NA, 1))
  y$sample_size <- o
  names(y) <- c("Factor Levels", "25Q", "Median", "75Q", "1.5XIQ", "# Outliers", "Ns")
  y <- y[order(abs(y$Median), decreasing = TRUE),]

print(xtable(y, caption="Relationship between Project and MPDS between different factor levels"), comment = FALSE)

  
rm(i, o, y, x)
```

Table 6 and figure 7 show no clear relationship between factor levels of project. Both factor levels apart from NONE have a tiny sample size, and generalization should happen carefully. Program enhancement seems to result in lower median sales, where enhancement likely means that existing parts of the building have had to close, as opposed to none and interfering with a location's ability to sell products.

\newpage

#### Outlier Detection and Relationships: Time Series of Sales per Site_ID aggregated per day

```{r}
sum_sales <- aggregate(df$sales, by=list(df$date, df$site_id), FUN=sum)
names(sum_sales) <- c("Date", "Site_ID", "Sum_of_Sales")
```

```{r, fig.height = 9, fig.width = 7, fig.cap='Sales over time'}
mycolors <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, "Set1"))(10)

ggplot(sum_sales, aes(x=Date, y=Sum_of_Sales, color=Site_ID)) +
  facet_wrap(~Site_ID, scale="free_y", nrow = 5, ncol = 2) +
  geom_point(position = "identity", stat = "identity", alpha = .8, color="grey", size=0.75) +
  geom_smooth(se=F) +
  theme_fivethirtyeight() +
  scale_color_manual(values = mycolors) +
  scale_y_continuous(labels = scales::dollar, limits = c(0,1500)) +
  theme(legend.position = ("none"), strip.text = element_text(size=15), plot.title = element_text(size=22),axis.title.x = element_text(size = 15), axis.title.y =     element_text(size = 15)) +
  labs(x = "Site_ID", y = "Occupation Growth in percentage", title = "Total Sales per Site_ID ")

```

```{r, results='asis'}
counter = 0

list_siteid <- NULL
list_mpds <- NULL
df2 <- NULL

for (i in c(levels(df$site_id))) {
   counter = counter+1
   list_siteid[counter] <- i
   df2 <- df %>% filter(site_id == i)
   list_mpds[counter] <- mean(as.numeric(as.character(df2$mpds)))
  
}

table2 <-data.frame(site_id = list_siteid, mpds = list_mpds)

xtable(table2, caption = "Amount of Pumps per Site_ID")

rm(table2, list_mpds, list_siteid, i, counter, df2)

```

Figure 8 shows the aggregate sum of sales per day seperated per site_id, or location.  There is a clear difference between various sites in the amount of aggregated sales per day. Every location shows differences in seasonality, e.g. 380 vs 280, and growth e.g. 280 vs 517. Table 7 shows the site_id and the matching amount of pumps. There is no apparent difference in the number of pumps and sales in a given day. Seasonality also does not appear to depend on the number of pumps available at the get station, and neither does growth.

/newpage

# Results:

Through the exploratory analysis we performed on our data, we have discovered the following:

1.  We cannot currently predict any values correctly without running a time-series analysis. After trying to use a standard GLM algorithm we found that it was not very descriptive of what was going on in our data. With that in mind we will be researching the best method for performing a time-series analysis and which platform is best for this use case (ie. Python or R).

2.  After mapping the supplementary data set to the original we found that there were 11,799 NA values contained within the temperature variable. We have a few thoughts on how to go about this but want to do some more research to find best practices. Some of our insights are to possibly use historical temperature data by site_id to fill in the NA values.

3.  Multicollinnearity is present in this data set and will need to be resolved for our final analysis. It exists between the max temp and the min temp, as well as the Gross Book Value and the Square Footage. For the temperature variables, our thoughts are to exclude Min Temp since that generally takes place during the middle of the night where Max Temp is a more accurate representation of the normal hours of operation. For GBV and sq_footage our thoughts are to create two separate variables to help standardize the data.


Currently as the data stands, we cannot derive much insight into the overall predictability of our variables using a general linear model. However, we were able to find issues within the data that will help us be more prepared for when we perform the time-series analysis. This EDA has helped us understand the different approaches we need to take both in pre-processing as well as our time-series method. As it stands, dealing with multicollinnearity and the NA values will help us in our overall predictability of our model.


# Team Member Contribution
Sam, Willem, and Kelsey produced an initial EDA working together. Tim also produced an initial EDA which was compared to the group EDA.  We merged the initial documents where it made sense.  Willem proceeded to build out more of the substantial code for the analysis. Kelsey, Willem, and Sam provided written analysis. Additionally, Kelsey and Sam proofread and provided final edits.  For the final collaboration, the whole team used GitHub to make final edits and adjustments to the EDA.
